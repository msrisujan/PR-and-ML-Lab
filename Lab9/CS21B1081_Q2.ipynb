{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.Sw = None\n",
    "        self.Sb = None\n",
    "        self.eigen_values = None\n",
    "        self.eigen_vectors = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        Sw = np.zeros((self.X_train.shape[1], self.X_train.shape[1]))\n",
    "        Sb = np.zeros((self.X_train.shape[1], self.X_train.shape[1]))\n",
    "        Sw = np.zeros((self.X_train.shape[1], self.X_train.shape[1]))\n",
    "        for i in range(len(self.X_train)):\n",
    "            x = self.X_train[i].reshape(self.X_train.shape[1], 1)\n",
    "            m = np.mean(self.X_train[self.y_train == self.y_train[i]])\n",
    "            Sw += np.dot((x - m), (x - m).T)\n",
    "        m = np.mean(self.X_train)\n",
    "        for i in range(len(self.X_train)):\n",
    "            x = self.X_train[i].reshape(self.X_train.shape[1], 1)\n",
    "            m_i = np.mean(self.X_train[self.y_train == self.y_train[i]])\n",
    "            Sb += len(self.X_train[self.y_train == self.y_train[i]]) * np.dot((m_i - m), (m_i - m).T)\n",
    "        self.Sw = Sw\n",
    "        self.Sb = Sb\n",
    "        A = np.dot(np.linalg.inv(Sw), Sb)\n",
    "        eigen_values, eigen_vectors = np.linalg.eig(A)\n",
    "        eigen_vectors = eigen_vectors.T\n",
    "        idx = np.argsort(abs(eigen_values))[::-1]\n",
    "        eigen_vectors = eigen_vectors[idx]\n",
    "        eigen_values = eigen_values[idx]\n",
    "        self.eigen_values = eigen_values[0:self.n_components]\n",
    "        self.eigen_vectors = eigen_vectors[0:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.dot(X, self.eigen_vectors.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(X_train, X_test):\n",
    "    dist = np.zeros((len(X_test), len(X_train)))\n",
    "    for i in range(len(X_test)):\n",
    "        for j in range(len(X_train)):\n",
    "            dist[i,j] = np.sqrt(np.sum((X_test[i] - X_train[j])**2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(X):\n",
    "    mean = np.mean(X,axis=0)\n",
    "    X = X - mean\n",
    "    return np.dot(X.T, X)/(X.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k = 5):\n",
    "        self.k = k\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        self.distances = euclidean_distance(self.X_train, X_test)\n",
    "        pred = []\n",
    "        for dist in self.distances:\n",
    "            k_nearest_indices = np.argsort(dist)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_nearest_indices]\n",
    "            pred.append(np.unique(k_nearest_labels)[np.argmax(np.unique(k_nearest_labels, return_counts=True)[1])])\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('face.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
      "1   0.541322  0.586777  0.640496  0.661157  0.685950  0.685950  0.690083   \n",
      "2   0.578512  0.603306  0.632231  0.665289  0.677686  0.710744  0.723140   \n",
      "3   0.169422  0.264463  0.219008  0.280992  0.421488  0.549587  0.669422   \n",
      "4   0.454545  0.429752  0.537190  0.611570  0.652893  0.702479  0.727273   \n",
      "5   0.719008  0.727273  0.723140  0.714876  0.723140  0.731405  0.739669   \n",
      "6   0.206612  0.280992  0.367769  0.392562  0.681818  0.714876  0.723140   \n",
      "7   0.185950  0.194215  0.322314  0.524793  0.661157  0.772727  0.822314   \n",
      "8   0.500000  0.487603  0.537190  0.574380  0.595041  0.595041  0.628099   \n",
      "9   0.326446  0.483471  0.524793  0.599174  0.665289  0.702479  0.702479   \n",
      "10  0.289256  0.239669  0.227273  0.528926  0.772727  0.826446  0.834711   \n",
      "11  0.483471  0.520661  0.590909  0.632231  0.652893  0.706612  0.706612   \n",
      "12  0.371901  0.487603  0.566116  0.586777  0.590909  0.615702  0.657025   \n",
      "13  0.632231  0.652893  0.582645  0.636364  0.636364  0.706612  0.747934   \n",
      "14  0.603306  0.586777  0.541322  0.603306  0.603306  0.607438  0.648760   \n",
      "15  0.632231  0.648760  0.636364  0.628099  0.648760  0.640496  0.648760   \n",
      "16  0.433884  0.466942  0.524793  0.533058  0.553719  0.553719  0.557851   \n",
      "17  0.524793  0.545455  0.512397  0.545455  0.586777  0.578512  0.632231   \n",
      "18  0.252066  0.243802  0.239669  0.301653  0.359504  0.479339  0.752066   \n",
      "19  0.479339  0.549587  0.628099  0.690083  0.677686  0.652893  0.640496   \n",
      "20  0.516529  0.512397  0.508265  0.516529  0.537190  0.566116  0.561983   \n",
      "21  0.136364  0.140496  0.185950  0.280992  0.318182  0.363636  0.409091   \n",
      "22  0.409091  0.590909  0.657025  0.681818  0.694215  0.731405  0.760331   \n",
      "23  0.433884  0.462810  0.500000  0.541322  0.537190  0.533058  0.524793   \n",
      "24  0.557851  0.628099  0.677686  0.702479  0.710744  0.743802  0.756198   \n",
      "25  0.615702  0.685950  0.714876  0.739669  0.735537  0.752066  0.752066   \n",
      "26  0.417355  0.471074  0.595041  0.710744  0.764463  0.768595  0.776860   \n",
      "27  0.611570  0.644628  0.657025  0.669422  0.673554  0.710744  0.735537   \n",
      "28  0.185950  0.128099  0.115702  0.132231  0.185950  0.268595  0.433884   \n",
      "29  0.438017  0.479339  0.512397  0.553719  0.574380  0.595041  0.623967   \n",
      "30  0.471074  0.508265  0.512397  0.520661  0.533058  0.549587  0.557851   \n",
      "31  0.384298  0.582645  0.710744  0.780992  0.834711  0.847107  0.847107   \n",
      "32  0.123967  0.099174  0.123967  0.107438  0.107438  0.128099  0.128099   \n",
      "33  0.446281  0.466942  0.471074  0.479339  0.520661  0.549587  0.574380   \n",
      "34  0.301653  0.309917  0.338843  0.367769  0.392562  0.466942  0.578512   \n",
      "35  0.235537  0.260331  0.392562  0.632231  0.826446  0.826446  0.814050   \n",
      "36  0.280992  0.194215  0.301653  0.487603  0.533058  0.541322  0.541322   \n",
      "37  0.123967  0.103306  0.115702  0.115702  0.111570  0.136364  0.185950   \n",
      "38  0.252066  0.219008  0.227273  0.272727  0.318182  0.388430  0.458678   \n",
      "39  0.545455  0.611570  0.640496  0.657025  0.636364  0.648760  0.690083   \n",
      "\n",
      "           7         8         9  ...      4087      4088      4089      4090  \\\n",
      "0   0.677686  0.690083  0.685950  ...  0.669422  0.652893  0.661157  0.475207   \n",
      "1   0.698347  0.694215  0.690083  ...  0.483471  0.495868  0.512397  0.528926   \n",
      "2   0.739669  0.739669  0.743802  ...  0.177686  0.194215  0.198347  0.206612   \n",
      "3   0.702479  0.706612  0.731405  ...  0.425620  0.438017  0.438017  0.454545   \n",
      "4   0.735537  0.735537  0.735537  ...  0.359504  0.380165  0.384298  0.388430   \n",
      "5   0.760331  0.780992  0.793388  ...  0.500000  0.438017  0.566116  0.743802   \n",
      "6   0.739669  0.752066  0.768595  ...  0.537190  0.537190  0.636364  0.285124   \n",
      "7   0.838843  0.842975  0.842975  ...  0.285124  0.268595  0.243802  0.239669   \n",
      "8   0.644628  0.661157  0.661157  ...  0.264463  0.268595  0.276859  0.227273   \n",
      "9   0.710744  0.731405  0.731405  ...  0.380165  0.305785  0.185950  0.090909   \n",
      "10  0.838843  0.842975  0.838843  ...  0.252066  0.194215  0.140496  0.165289   \n",
      "11  0.743802  0.780992  0.785124  ...  0.516529  0.487603  0.396694  0.152893   \n",
      "12  0.665289  0.673554  0.685950  ...  0.280992  0.297521  0.351240  0.396694   \n",
      "13  0.780992  0.793388  0.814050  ...  0.285124  0.272727  0.210744  0.231405   \n",
      "14  0.640496  0.607438  0.599174  ...  0.557851  0.553719  0.537190  0.541322   \n",
      "15  0.628099  0.599174  0.615702  ...  0.363636  0.409091  0.586777  0.561983   \n",
      "16  0.578512  0.619835  0.640496  ...  0.214876  0.190083  0.181818  0.177686   \n",
      "17  0.661157  0.694215  0.731405  ...  0.301653  0.326446  0.326446  0.342975   \n",
      "18  0.826446  0.851240  0.859504  ...  0.512397  0.504132  0.504132  0.388430   \n",
      "19  0.636364  0.648760  0.648760  ...  0.214876  0.219008  0.219008  0.223140   \n",
      "20  0.574380  0.578512  0.582645  ...  0.086777  0.099174  0.136364  0.334711   \n",
      "21  0.417355  0.438017  0.454545  ...  0.169422  0.206612  0.276859  0.355372   \n",
      "22  0.785124  0.776860  0.785124  ...  0.276859  0.314050  0.268595  0.247934   \n",
      "23  0.533058  0.549587  0.549587  ...  0.595041  0.462810  0.355372  0.235537   \n",
      "24  0.764463  0.785124  0.789256  ...  0.132231  0.293388  0.322314  0.392562   \n",
      "25  0.739669  0.760331  0.780992  ...  0.247934  0.272727  0.367769  0.297521   \n",
      "26  0.801653  0.818182  0.822314  ...  0.074380  0.074380  0.078512  0.074380   \n",
      "27  0.756198  0.760331  0.768595  ...  0.152893  0.107438  0.119835  0.132231   \n",
      "28  0.541322  0.570248  0.586777  ...  0.396694  0.409091  0.301653  0.421488   \n",
      "29  0.632231  0.652893  0.673554  ...  0.566116  0.446281  0.338843  0.289256   \n",
      "30  0.566116  0.582645  0.578512  ...  0.272727  0.301653  0.347107  0.388430   \n",
      "31  0.855372  0.838843  0.842975  ...  0.140496  0.148760  0.185950  0.161157   \n",
      "32  0.132231  0.148760  0.148760  ...  0.082645  0.165289  0.404959  0.632231   \n",
      "33  0.603306  0.615702  0.611570  ...  0.384298  0.417355  0.413223  0.384298   \n",
      "34  0.636364  0.661157  0.673554  ...  0.603306  0.615702  0.673554  0.681818   \n",
      "35  0.822314  0.826446  0.768595  ...  0.475207  0.628099  0.582645  0.210744   \n",
      "36  0.582645  0.615702  0.603306  ...  0.190083  0.194215  0.202479  0.140496   \n",
      "37  0.334711  0.512397  0.611570  ...  0.479339  0.471074  0.479339  0.491736   \n",
      "38  0.504132  0.516529  0.524793  ...  0.260331  0.276859  0.293388  0.206612   \n",
      "39  0.669422  0.665289  0.710744  ...  0.169422  0.173554  0.148760  0.169422   \n",
      "\n",
      "        4091      4092      4093      4094      4095  target  \n",
      "0   0.132231  0.148760  0.152893  0.161157  0.157025       0  \n",
      "1   0.363636  0.111570  0.095041  0.111570  0.111570       1  \n",
      "2   0.194215  0.165289  0.177686  0.161157  0.152893       2  \n",
      "3   0.363636  0.198347  0.210744  0.235537  0.214876       3  \n",
      "4   0.376033  0.409091  0.409091  0.384298  0.384298       4  \n",
      "5   0.566116  0.314050  0.301653  0.338843  0.322314       5  \n",
      "6   0.235537  0.231405  0.252066  0.243802  0.272727       6  \n",
      "7   0.219008  0.202479  0.190083  0.194215  0.202479       7  \n",
      "8   0.239669  0.264463  0.293388  0.293388  0.280992       8  \n",
      "9   0.086777  0.074380  0.082645  0.066116  0.078512       9  \n",
      "10  0.301653  0.169422  0.095041  0.107438  0.111570      10  \n",
      "11  0.140496  0.140496  0.136364  0.115702  0.099174      11  \n",
      "12  0.421488  0.438017  0.371901  0.619835  0.590909      12  \n",
      "13  0.223140  0.107438  0.082645  0.090909  0.090909      13  \n",
      "14  0.533058  0.504132  0.528926  0.495868  0.495868      14  \n",
      "15  0.190083  0.165289  0.202479  0.198347  0.198347      15  \n",
      "16  0.169422  0.177686  0.173554  0.169422  0.169422      16  \n",
      "17  0.347107  0.351240  0.363636  0.355372  0.371901      17  \n",
      "18  0.152893  0.157025  0.181818  0.161157  0.190083      18  \n",
      "19  0.223140  0.227273  0.223140  0.219008  0.227273      19  \n",
      "20  0.520661  0.557851  0.561983  0.557851  0.566116      20  \n",
      "21  0.685950  0.661157  0.280992  0.371901  0.276859      21  \n",
      "22  0.256198  0.264463  0.252066  0.276859  0.285124      22  \n",
      "23  0.210744  0.210744  0.231405  0.223140  0.210744      23  \n",
      "24  0.462810  0.475207  0.446281  0.272727  0.392562      24  \n",
      "25  0.132231  0.057851  0.115702  0.157025  0.185950      25  \n",
      "26  0.086777  0.111570  0.148760  0.152893  0.206612      26  \n",
      "27  0.111570  0.136364  0.177686  0.210744  0.223140      27  \n",
      "28  0.462810  0.247934  0.190083  0.210744  0.210744      28  \n",
      "29  0.285124  0.285124  0.293388  0.285124  0.289256      29  \n",
      "30  0.409091  0.450413  0.475207  0.450413  0.433884      30  \n",
      "31  0.190083  0.132231  0.219008  0.185950  0.210744      31  \n",
      "32  0.603306  0.177686  0.479339  0.619835  0.615702      32  \n",
      "33  0.557851  0.694215  0.644628  0.636364  0.475207      33  \n",
      "34  0.690083  0.714876  0.731405  0.714876  0.673554      34  \n",
      "35  0.206612  0.210744  0.227273  0.239669  0.239669      35  \n",
      "36  0.181818  0.247934  0.235537  0.305785  0.363636      36  \n",
      "37  0.487603  0.487603  0.458678  0.450413  0.442149      37  \n",
      "38  0.128099  0.136364  0.157025  0.157025  0.185950      38  \n",
      "39  0.165289  0.157025  0.173554  0.173554  0.181818      39  \n",
      "\n",
      "[40 rows x 4097 columns]\n"
     ]
    }
   ],
   "source": [
    "classes = df['target'].unique()\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for i in classes:\n",
    "    train = pd.concat([train, df[df['target'] == i].iloc[1:]], ignore_index=True)\n",
    "    test = pd.concat([test, df[df['target'] == i].iloc[:1]], ignore_index=True)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['target'], axis=1).values\n",
    "y_train = train['target'].values\n",
    "X_test = test.drop(['target'], axis=1).values\n",
    "y_test = test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(39)\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 360\n",
      "X_test: 40\n",
      "[[-0.06934221+0.j          0.02001728+0.j         -0.02876857+0.j\n",
      "  ...  0.09527241-0.03477702j  0.01238876+0.j\n",
      "  -0.16388612+0.04211298j]\n",
      " [-0.06934221+0.j          0.03288809+0.j         -0.02099824+0.j\n",
      "  ...  0.07803182-0.07746382j  0.04613435+0.j\n",
      "  -0.09455975-0.00403091j]\n",
      " [-0.06934221+0.j          0.03673326+0.j         -0.03528519+0.j\n",
      "  ...  0.06704287-0.04265419j  0.02799889+0.j\n",
      "   0.01290651+0.05458445j]\n",
      " ...\n",
      " [-0.06509287+0.j         -0.00996599+0.j         -0.01762707+0.j\n",
      "  ...  0.17846456+0.07167398j  0.15052054+0.j\n",
      "  -0.15965584-0.0493058j ]\n",
      " [-0.06509287+0.j          0.0292918 +0.j         -0.01161092+0.j\n",
      "  ...  0.01905974-0.06547205j  0.02042639+0.j\n",
      "   0.02920411+0.06245979j]\n",
      " [-0.06509287+0.j         -0.00365166+0.j          0.01126837+0.j\n",
      "  ...  0.23007053+0.05904044j  0.27063358+0.j\n",
      "  -0.14224525+0.00017954j]]\n"
     ]
    }
   ],
   "source": [
    "X_train = lda.transform(X_train)\n",
    "X_test = lda.transform(X_test)\n",
    "print(f\"X_train: {len(X_train)}\")\n",
    "print(f\"X_test: {len(X_test)}\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(5)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Predicted\n",
      "0        0         18\n",
      "1        1         26\n",
      "2        2          3\n",
      "3        3          1\n",
      "4        4          1\n",
      "5        5         13\n",
      "6        6          2\n",
      "7        7          7\n",
      "8        8         22\n",
      "9        9         29\n",
      "10      10          3\n",
      "11      11          3\n",
      "12      12         12\n",
      "13      13         13\n",
      "14      14          6\n",
      "15      15         35\n",
      "16      16         22\n",
      "17      17         17\n",
      "18      18          2\n",
      "19      19         29\n",
      "20      20          3\n",
      "21      21         37\n",
      "22      22          8\n",
      "23      23         26\n",
      "24      24         14\n",
      "25      25          6\n",
      "26      26          1\n",
      "27      27         36\n",
      "28      28          8\n",
      "29      29          3\n",
      "30      30         13\n",
      "31      31         26\n",
      "32      32         32\n",
      "33      33         26\n",
      "34      34         34\n",
      "35      35          2\n",
      "36      36         36\n",
      "37      37         26\n",
      "38      38          3\n",
      "39      39         12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8164/27565779.py:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  dist[i,j] = np.sqrt(np.sum((X_test[i] - X_train[j])**2))\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "compared = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(compared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.175\n",
      "accuracy percentage:  17.5 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_pred,y_test):\n",
    "    return np.sum(y_pred == y_test)/len(y_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_pred, y_test))\n",
    "print(\"accuracy percentage: \", accuracy(y_pred, y_test)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
